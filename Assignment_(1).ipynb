{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skevin-dev/NLP-FELLOWSHIP/blob/week5/Assignment_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6acSYAKEGxTa"
      },
      "source": [
        "# First Model Assignment\n",
        "The aim of this assignment is to make sure you understand the foundations of model training. We have covered traditional ML and also simple NN. You task is \n",
        "1. to write code for training traditional ML model that gives the highest accuracy\n",
        "2. Code for NN model that gives the highest accuracy\n",
        "\n",
        "## This we will consider\n",
        "1. The code works\n",
        "2. The hyperparameter used for fine tuning - epoch only is not enough\n",
        "3. The highest accuracy\n",
        "4. Bonus points for being creative with preprocessing, tokenization and creation of vectors\n",
        "\n",
        "# Submissions\n",
        "1. Notebook with code\n",
        "2. 2 models\n",
        "\n",
        "# Deadline\n",
        "Monday 28th at 5pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewCYwtguDWoM",
        "outputId": "bf13f1c8-8b57-45ff-f26b-694fe3ec79c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 959 kB/s \n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=3b9143186a0ebffde7a9dfd47ff1fa5cd6de41e97c5f9e287ed0db40f9733b23\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVt11IMVZqxx",
        "outputId": "7ca85c06-798a-4d1d-9963-af8e67900f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 16 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.0 torchtext==0.9.0 #compatibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRcYW332lAb"
      },
      "source": [
        "## Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULAfSIu02nHj",
        "outputId": "e8f53bf9-ca20-4e54-8499-621689b4cd62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import os \n",
        "import re\n",
        "import string \n",
        "from cleantext import clean\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "#models \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "#accuracy\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1P3_vu9-uKn",
        "outputId": "379a8dd6-a2da-4255-aba2-20ccb4bb5664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vab61iC9-u4a"
      },
      "outputs": [],
      "source": [
        "#accessing data directoty \n",
        "os.chdir('/content/drive/MyDrive/NLP Fellowship /data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wed5Z_Uq_C4z",
        "outputId": "e96695c2-eab4-4036-d586-7c8551b34e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50k_imdb_movie_reviews.csv  Eng_Kin-Paralleldata.csv\n",
            "article.txt\t\t    saved_weights_linear.pt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX12ImNz_Evx"
      },
      "outputs": [],
      "source": [
        "#import data \n",
        "df = pd.read_csv('50k_imdb_movie_reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJDETUcp_NUj",
        "outputId": "c4eef0a9-af5d-458c-86f1-e4d989ce2e5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#shape of the dataset to be used \n",
        "df.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQQvUCKSIvB5",
        "outputId": "89074d06-2b4b-4e6f-f538-0dfb012ffb47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "418"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking and removing duplicates \n",
        "df.duplicated(subset =['review']).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQCLdvwOKO5m"
      },
      "outputs": [],
      "source": [
        "# drop duplicates \n",
        "df.drop_duplicates(subset=['review'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "S1qrErhi_OTH",
        "outputId": "ef3e1934-8795-408d-be55-48678c646b94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7940488c-ef69-46a0-927d-f0f901e8c968\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7940488c-ef69-46a0-927d-f0f901e8c968')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7940488c-ef69-46a0-927d-f0f901e8c968 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7940488c-ef69-46a0-927d-f0f901e8c968');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              review  sentiment   set\n",
              "0  I went and saw this movie last night after bei...          1  test\n",
              "1  Actor turned director Bill Paxton follows up h...          1  test\n",
              "2  As a recreational golfer with some knowledge o...          1  test\n",
              "3  I saw this film in a sneak preview, and it is ...          1  test\n",
              "4  Bill Paxton has taken the true story of the 19...          1  test"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#head of the dataset \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "H8vaNkzbC88z",
        "outputId": "6b2da834-ec21-4859-ab5b-3d9ab12fc75c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bill Paxton has taken the true story of the 1913 US golf open and made a film that is about much more than an extra-ordinary game of golf. The film also deals directly with the class tensions of the early twentieth century and touches upon the profound anti-Catholic prejudices of both the British and American establishments. But at heart the film is about that perennial favourite of triumph against the odds.<br /><br />The acting is exemplary throughout. Stephen Dillane is excellent as usual, but the revelation of the movie is Shia LaBoeuf who delivers a disciplined, dignified and highly sympathetic performance as a working class Franco-Irish kid fighting his way through the prejudices of the New England WASP establishment. For those who are only familiar with his slap-stick performances in \"Even Stevens\" this demonstration of his maturity is a delightful surprise. And Josh Flitter as the ten year old caddy threatens to steal every scene in which he appears.<br /><br />A old fashioned movie in the best sense of the word: fine acting, clear directing and a great story that grips to the end - the final scene an affectionate nod to Casablanca is just one of the many pleasures that fill a great movie.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'][4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulK3XHhmDBsI"
      },
      "source": [
        "we can see it contains some numbers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGan5urRACnc"
      },
      "source": [
        "## data cleaning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGdGfyTwAEwZ"
      },
      "outputs": [],
      "source": [
        "#using cleaning function \n",
        "def cleaning_text(text):\n",
        "  \"\"\"This functions helps to make some cleanings given a text file \n",
        "\n",
        "  Args\n",
        "  ----\n",
        "  text(str): text to be cleaned \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  final_text(list): returns a list containing cleaned text  \n",
        "  \"\"\"\n",
        "\n",
        "  # convert text file into a list and remove empty lines \n",
        "  uncleaned_text = [line for line in text.splitlines() if line.strip()]\n",
        "\n",
        "  # same cases \n",
        "  same_case = [line.lower() for line in uncleaned_text]\n",
        "\n",
        "  # remove duplicate lines \n",
        "  no_dups_line = []\n",
        "  for line in same_case:\n",
        "    if line not in no_dups_line:\n",
        "      no_dups_line.append(line)\n",
        "\n",
        "  # remove emails and urls\n",
        "  email_link_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "  no_email_link = [re.sub(email_link_pattern,\"\",line) for line in no_dups_line ]\n",
        "\n",
        "  # remove punctuations\n",
        "  no_punct = [line.translate(str.maketrans(\" \",\" \",string.punctuation)) for line in no_email_link]\n",
        "\n",
        "  # remove emojis \n",
        "  no_emojis = [clean(line,no_emoji=True) for line in no_punct]\n",
        "\n",
        "  # remove sensitive data \n",
        "  sensitive_data_pattern =  r'\\d{1,}|\\+\\d+\\s\\d+\\s\\d+|\\+\\d+\\s\\d+'\n",
        "\n",
        "  no_sensitive_data = [re.sub(sensitive_data_pattern,\"\",line) for line in no_emojis ]\n",
        "\n",
        "  # remove white spaces \n",
        "  final_text = [\" \".join(line.split()) for line in no_sensitive_data]\n",
        "\n",
        "  final_text_text  = final_text[0]\n",
        "\n",
        "  return final_text_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG7nSc-2AjBl"
      },
      "outputs": [],
      "source": [
        "#apply data cleaning function on review column\n",
        "df['review'] = df['review'].apply(cleaning_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ypeWg7gWC3sN",
        "outputId": "51114c26-5cea-43e4-d651-6af5e1ba5990"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bill paxton has taken the true story of the us golf open and made a film that is about much more than an extraordinary game of golf the film also deals directly with the class tensions of the early twentieth century and touches upon the profound anticatholic prejudices of both the british and american establishments but at heart the film is about that perennial favourite of triumph against the oddsbr br the acting is exemplary throughout stephen dillane is excellent as usual but the revelation of the movie is shia laboeuf who delivers a disciplined dignified and highly sympathetic performance as a working class francoirish kid fighting his way through the prejudices of the new england wasp establishment for those who are only familiar with his slapstick performances in even stevens this demonstration of his maturity is a delightful surprise and josh flitter as the ten year old caddy threatens to steal every scene in which he appearsbr br a old fashioned movie in the best sense of the word fine acting clear directing and a great story that grips to the end the final scene an affectionate nod to casablanca is just one of the many pleasures that fill a great movie'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check \n",
        "df['review'][4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34kk9rZ2DysJ"
      },
      "source": [
        "we can see that it is cleaned "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFKYtEJUOvib",
        "outputId": "d47b242c-6475-41fa-f8f2-fce27f6241cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of training dataset is (24781, 3)\n",
            "the shape of testing dataset is (24801, 3)\n"
          ]
        }
      ],
      "source": [
        "# spliting training dataset and testing dataset \n",
        "training_dataset =  df[df['set'] == 'train']\n",
        "testing_dataset = df[df['set']=='test']\n",
        "print(f'the shape of training dataset is {training_dataset.shape}')\n",
        "print(f'the shape of testing dataset is {testing_dataset.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuZlbov5Pjf6",
        "outputId": "a369d4f1-d961-4730-d5b8-fb333c7cadda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "source": [
        "# delete set columns \n",
        "del training_dataset['set']\n",
        "testing_dataset.drop('set',axis =1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "livoS4KdQFkf",
        "outputId": "d9b4e0b9-f901-49c1-9154-d7ef5d39b15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of training dataset is (24781, 2)\n",
            "the shape of testing dataset is (24801, 2)\n"
          ]
        }
      ],
      "source": [
        "print(f'the shape of training dataset is {training_dataset.shape}')\n",
        "print(f'the shape of testing dataset is {testing_dataset.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC2N2nrAQmNr",
        "outputId": "8e4f0b0a-6724-467c-890f-0ef9968537dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 24781 entries, 25000 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     24781 non-null  object\n",
            " 1   sentiment  24781 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 580.8+ KB\n"
          ]
        }
      ],
      "source": [
        "training_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12Sixw4GQWPb"
      },
      "outputs": [],
      "source": [
        "# spliting data into training and validation \n",
        "X,y = training_dataset['review'],training_dataset['sentiment']\n",
        "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.20,random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2ulMp0cWE6c"
      },
      "outputs": [],
      "source": [
        "# vectorization using TF-IDF \n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec =  vectorizer.fit_transform(X_train)\n",
        "joblib.dump(vectorizer,'/content/drive/MyDrive/NLP Fellowship /Models/TFIDFvector.pkl')\n",
        "X_val_vec = vectorizer.transform(X_val)\n",
        "X_test_vec = vectorizer.transform(testing_dataset['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2qRi1rm0EYh",
        "outputId": "29921908-4dbc-4f00-9b74-4938c64ea0cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19824, 102865)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jWaNuvL0B2E"
      },
      "outputs": [],
      "source": [
        "# Vectorization using countvector\n",
        "vc = CountVectorizer()\n",
        "X_train_vc = vc.fit_transform(X_train)\n",
        "joblib.dump(vc,'/content/drive/MyDrive/NLP Fellowship /Models/countvector.pkl')\n",
        "X_val_vc = vc.transform(X_val)\n",
        "X_test_vc = vc.transform(testing_dataset['review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kusvaZ3vYcbh"
      },
      "source": [
        "## Tuning tradition ML model and their parameters using Scikit-learn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvuLwcv11Cqe"
      },
      "outputs": [],
      "source": [
        "n_estimators = [i for i in range(10,110,10)]\n",
        "criterion = ['gini','entropy']\n",
        "max_depth = [i for i in range(1,11)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFTWZjlB2YMC",
        "outputId": "b252c654-8af7-4014-a706-6b6bbe615bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parameters:(10,gini,1); trainining accuracy:0.5658797417271993; validation accuracy:0.5755497276578576\n",
            "parameters:(10,gini,2); trainining accuracy:0.6211158192090396; validation accuracy:0.6114585434738753\n",
            "parameters:(10,gini,3); trainining accuracy:0.6262106537530266; validation accuracy:0.6211418196489812\n",
            "parameters:(10,gini,4); trainining accuracy:0.7004640839386602; validation accuracy:0.6887230179544079\n",
            "parameters:(10,gini,5); trainining accuracy:0.6949656981436643; validation accuracy:0.6865039338309461\n",
            "parameters:(10,gini,6); trainining accuracy:0.7358757062146892; validation accuracy:0.724430098850111\n",
            "parameters:(10,gini,7); trainining accuracy:0.7278551251008878; validation accuracy:0.6986080290498285\n",
            "parameters:(10,gini,8); trainining accuracy:0.7306295399515739; validation accuracy:0.6943715957232197\n",
            "parameters:(10,gini,9); trainining accuracy:0.7487389023405973; validation accuracy:0.7177728464797256\n",
            "parameters:(10,gini,10); trainining accuracy:0.768815577078289; validation accuracy:0.7183780512406698\n",
            "parameters:(10,entropy,1); trainining accuracy:0.5640133171912833; validation accuracy:0.5664716562436958\n",
            "parameters:(10,entropy,2); trainining accuracy:0.6314063761097659; validation accuracy:0.6257817228162195\n",
            "parameters:(10,entropy,3); trainining accuracy:0.6354923325262308; validation accuracy:0.6112568085535606\n",
            "parameters:(10,entropy,4); trainining accuracy:0.6977905569007264; validation accuracy:0.679039741779302\n",
            "parameters:(10,entropy,5); trainining accuracy:0.692090395480226; validation accuracy:0.6608835989509784\n",
            "parameters:(10,entropy,6); trainining accuracy:0.6976896690879741; validation accuracy:0.6798466814605608\n",
            "parameters:(10,entropy,7); trainining accuracy:0.7295197740112994; validation accuracy:0.704458341738955\n",
            "parameters:(10,entropy,8); trainining accuracy:0.741727199354318; validation accuracy:0.7137381480734315\n",
            "parameters:(10,entropy,9); trainining accuracy:0.7594330104923325; validation accuracy:0.7103086544280816\n",
            "parameters:(10,entropy,10); trainining accuracy:0.7350686037126715; validation accuracy:0.6959854750857374\n",
            "parameters:(20,gini,1); trainining accuracy:0.6212167070217918; validation accuracy:0.6255799878959047\n",
            "parameters:(20,gini,2); trainining accuracy:0.6651533494753834; validation accuracy:0.6390962275569901\n",
            "parameters:(20,gini,3); trainining accuracy:0.670046408393866; validation accuracy:0.6588662497478314\n",
            "parameters:(20,gini,4); trainining accuracy:0.7179681194511703; validation accuracy:0.699616703651402\n",
            "parameters:(20,gini,5); trainining accuracy:0.74909200968523; validation accuracy:0.7262457131329433\n",
            "parameters:(20,gini,6); trainining accuracy:0.7636702986279258; validation accuracy:0.7387532781924551\n",
            "parameters:(20,gini,7); trainining accuracy:0.7631154156577885; validation accuracy:0.7193867258422433\n",
            "parameters:(20,gini,8); trainining accuracy:0.7920197740112994; validation accuracy:0.741577567076861\n",
            "parameters:(20,gini,9); trainining accuracy:0.8013518966908797; validation accuracy:0.7571111559410934\n",
            "parameters:(20,gini,10); trainining accuracy:0.7982243744955609; validation accuracy:0.7452087956425257\n",
            "parameters:(20,entropy,1); trainining accuracy:0.5962974172719936; validation accuracy:0.5937058704861812\n",
            "parameters:(20,entropy,2); trainining accuracy:0.669138418079096; validation accuracy:0.6590679846681461\n",
            "parameters:(20,entropy,3); trainining accuracy:0.6823042776432607; validation accuracy:0.6671373814807343\n",
            "parameters:(20,entropy,4); trainining accuracy:0.7337570621468926; validation accuracy:0.719588460762558\n",
            "parameters:(20,entropy,5); trainining accuracy:0.7402138821630347; validation accuracy:0.7189832560016138\n",
            "parameters:(20,entropy,6); trainining accuracy:0.7571630347054076; validation accuracy:0.732701230583014\n",
            "parameters:(20,entropy,7); trainining accuracy:0.7625100887812752; validation accuracy:0.7302804115392374\n",
            "parameters:(20,entropy,8); trainining accuracy:0.7744652945924132; validation accuracy:0.7375428686705668\n",
            "parameters:(20,entropy,9); trainining accuracy:0.7951472962066183; validation accuracy:0.7490417591285051\n",
            "parameters:(20,entropy,10); trainining accuracy:0.8047820823244553; validation accuracy:0.7472261448456727\n",
            "parameters:(30,gini,1); trainining accuracy:0.6266142050040355; validation accuracy:0.61851926568489\n",
            "parameters:(30,gini,2); trainining accuracy:0.6815980629539952; validation accuracy:0.6764171878152108\n",
            "parameters:(30,gini,3); trainining accuracy:0.7344128329297821; validation accuracy:0.7115190639499698\n",
            "parameters:(30,gini,4); trainining accuracy:0.7244249394673123; validation accuracy:0.6911438369981844\n",
            "parameters:(30,gini,5); trainining accuracy:0.7687146892655368; validation accuracy:0.7488400242081904\n",
            "parameters:(30,gini,6); trainining accuracy:0.7849071832122679; validation accuracy:0.7472261448456727\n",
            "parameters:(30,gini,7); trainining accuracy:0.795046408393866; validation accuracy:0.7559007464192051\n",
            "parameters:(30,gini,8); trainining accuracy:0.811541565778854; validation accuracy:0.7746620940084729\n",
            "parameters:(30,gini,9); trainining accuracy:0.8146186440677966; validation accuracy:0.7607423845067581\n",
            "parameters:(30,gini,10); trainining accuracy:0.8334846650524617; validation accuracy:0.7837401654226347\n",
            "parameters:(30,entropy,1); trainining accuracy:0.6437651331719129; validation accuracy:0.6346580593100666\n",
            "parameters:(30,entropy,2); trainining accuracy:0.7095944309927361; validation accuracy:0.6865039338309461\n",
            "parameters:(30,entropy,3); trainining accuracy:0.7317897497982244; validation accuracy:0.7304821464595521\n",
            "parameters:(30,entropy,4); trainining accuracy:0.7528753026634383; validation accuracy:0.7361307242283639\n",
            "parameters:(30,entropy,5); trainining accuracy:0.7716908797417272; validation accuracy:0.7530764575347992\n",
            "parameters:(30,entropy,6); trainining accuracy:0.7869753833736884; validation accuracy:0.7609441194270729\n",
            "parameters:(30,entropy,7); trainining accuracy:0.8091202582728007; validation accuracy:0.7754690336897317\n",
            "parameters:(30,entropy,8); trainining accuracy:0.807001614205004; validation accuracy:0.7680048416380876\n",
            "parameters:(30,entropy,9); trainining accuracy:0.8018058918482648; validation accuracy:0.7522695178535405\n",
            "parameters:(30,entropy,10); trainining accuracy:0.8266242937853108; validation accuracy:0.7700221908412346\n",
            "parameters:(40,gini,1); trainining accuracy:0.6674233252623083; validation accuracy:0.6703651402057696\n",
            "parameters:(40,gini,2); trainining accuracy:0.7114608555286521; validation accuracy:0.7032479322170668\n",
            "parameters:(40,gini,3); trainining accuracy:0.7263418079096046; validation accuracy:0.7109138591890256\n",
            "parameters:(40,gini,4); trainining accuracy:0.7772397094430993; validation accuracy:0.7615493241880169\n",
            "parameters:(40,gini,5); trainining accuracy:0.7782990314769975; validation accuracy:0.7530764575347992\n",
            "parameters:(40,gini,6); trainining accuracy:0.8087671509281679; validation accuracy:0.7756707686100464\n",
            "parameters:(40,gini,7); trainining accuracy:0.8091202582728007; validation accuracy:0.7625579987895905\n",
            "parameters:(40,gini,8); trainining accuracy:0.8200665859564165; validation accuracy:0.7756707686100464\n",
            "parameters:(40,gini,9); trainining accuracy:0.8303066989507667; validation accuracy:0.7708291305224935\n",
            "parameters:(40,gini,10); trainining accuracy:0.8456416464891041; validation accuracy:0.78353843050232\n",
            "parameters:(40,entropy,1); trainining accuracy:0.6667675544794189; validation accuracy:0.6598749243494049\n",
            "parameters:(40,entropy,2); trainining accuracy:0.7257869249394673; validation accuracy:0.7197901956828727\n",
            "parameters:(40,entropy,3); trainining accuracy:0.7584745762711864; validation accuracy:0.7292717369376639\n",
            "parameters:(40,entropy,4); trainining accuracy:0.778500807102502; validation accuracy:0.7611458543473876\n",
            "parameters:(40,entropy,5); trainining accuracy:0.7792070217917676; validation accuracy:0.753278192455114\n",
            "parameters:(40,entropy,6); trainining accuracy:0.7933313155770783; validation accuracy:0.7643736130724228\n",
            "parameters:(40,entropy,7); trainining accuracy:0.804731638418079; validation accuracy:0.7730482146459552\n",
            "parameters:(40,entropy,8); trainining accuracy:0.8200665859564165; validation accuracy:0.7756707686100464\n",
            "parameters:(40,entropy,9); trainining accuracy:0.8335855528652139; validation accuracy:0.7845471051038935\n",
            "parameters:(40,entropy,10); trainining accuracy:0.8454903147699758; validation accuracy:0.7885818035101876\n",
            "parameters:(50,gini,1); trainining accuracy:0.6855326876513317; validation accuracy:0.6719790195682873\n",
            "parameters:(50,gini,2); trainining accuracy:0.7222054075867635; validation accuracy:0.7092999798265079\n",
            "parameters:(50,gini,3); trainining accuracy:0.7809221146085553; validation accuracy:0.7724430098850111\n",
            "parameters:(50,gini,4); trainining accuracy:0.7778954802259888; validation accuracy:0.7468226750050434\n",
            "parameters:(50,gini,5); trainining accuracy:0.7988801452784504; validation accuracy:0.7694169860802905\n",
            "parameters:(50,gini,6); trainining accuracy:0.8105831315577078; validation accuracy:0.7885818035101876\n",
            "parameters:(50,gini,7); trainining accuracy:0.8159806295399515; validation accuracy:0.7760742384506758\n",
            "parameters:(50,gini,8); trainining accuracy:0.8278853914447135; validation accuracy:0.7831349606616905\n",
            "parameters:(50,gini,9); trainining accuracy:0.851089588377724; validation accuracy:0.7956425257212023\n",
            "parameters:(50,gini,10); trainining accuracy:0.8516949152542372; validation accuracy:0.7976598749243494\n",
            "parameters:(50,entropy,1); trainining accuracy:0.6608151735270379; validation accuracy:0.6421222513617107\n",
            "parameters:(50,entropy,2); trainining accuracy:0.7432405165456013; validation accuracy:0.7284647972564051\n",
            "parameters:(50,entropy,3); trainining accuracy:0.7599374495560937; validation accuracy:0.7524712527738552\n",
            "parameters:(50,entropy,4); trainining accuracy:0.7930286521388217; validation accuracy:0.77667944321162\n",
            "parameters:(50,entropy,5); trainining accuracy:0.8015536723163842; validation accuracy:0.7823280209804317\n",
            "parameters:(50,entropy,6); trainining accuracy:0.8142150928167877; validation accuracy:0.7788985273350817\n",
            "parameters:(50,entropy,7); trainining accuracy:0.83454398708636; validation accuracy:0.7978616098446641\n",
            "parameters:(50,entropy,8); trainining accuracy:0.8292978208232445; validation accuracy:0.7871696590679846\n",
            "parameters:(50,entropy,9); trainining accuracy:0.8323244552058111; validation accuracy:0.7885818035101876\n",
            "parameters:(50,entropy,10); trainining accuracy:0.8526533494753834; validation accuracy:0.7962477304821465\n",
            "parameters:(60,gini,1); trainining accuracy:0.7005145278450363; validation accuracy:0.6863021989106314\n",
            "parameters:(60,gini,2); trainining accuracy:0.7360774818401937; validation accuracy:0.7189832560016138\n",
            "parameters:(60,gini,3); trainining accuracy:0.7696731234866828; validation accuracy:0.7403671575549727\n",
            "parameters:(60,gini,4); trainining accuracy:0.7990314769975787; validation accuracy:0.7728464797256405\n",
            "parameters:(60,gini,5); trainining accuracy:0.8111380145278451; validation accuracy:0.7825297559007465\n",
            "parameters:(60,gini,6); trainining accuracy:0.8169390637610977; validation accuracy:0.7823280209804317\n",
            "parameters:(60,gini,7); trainining accuracy:0.8293482647296206; validation accuracy:0.7926165019164817\n",
            "parameters:(60,gini,8); trainining accuracy:0.8452885391444713; validation accuracy:0.79382691143837\n",
            "parameters:(60,gini,9); trainining accuracy:0.8518966908797417; validation accuracy:0.7982650796852935\n",
            "parameters:(60,gini,10); trainining accuracy:0.8553773204196933; validation accuracy:0.7990720193665524\n",
            "parameters:(60,entropy,1); trainining accuracy:0.6971852300242131; validation accuracy:0.681057090982449\n",
            "parameters:(60,entropy,2); trainining accuracy:0.7402138821630347; validation accuracy:0.7266491829735727\n",
            "parameters:(60,entropy,3); trainining accuracy:0.7764326069410815; validation accuracy:0.7619527940286464\n",
            "parameters:(60,entropy,4); trainining accuracy:0.7947437449556094; validation accuracy:0.777284647972564\n",
            "parameters:(60,entropy,5); trainining accuracy:0.8005447941888619; validation accuracy:0.7744603590881581\n",
            "parameters:(60,entropy,6); trainining accuracy:0.8098769168684423; validation accuracy:0.7819245511398023\n",
            "parameters:(60,entropy,7); trainining accuracy:0.8293987086359967; validation accuracy:0.7875731289086141\n",
            "parameters:(60,entropy,8); trainining accuracy:0.8426654560129136; validation accuracy:0.802703247932217\n",
            "parameters:(60,entropy,9); trainining accuracy:0.8509887005649718; validation accuracy:0.7964494654024612\n",
            "parameters:(60,entropy,10); trainining accuracy:0.8531073446327684; validation accuracy:0.7968529352430905\n",
            "parameters:(70,gini,1); trainining accuracy:0.7204903147699758; validation accuracy:0.7151502925156344\n",
            "parameters:(70,gini,2); trainining accuracy:0.7526735270379338; validation accuracy:0.7421827718378051\n",
            "parameters:(70,gini,3); trainining accuracy:0.7730528652138822; validation accuracy:0.7609441194270729\n",
            "parameters:(70,gini,4); trainining accuracy:0.7909100080710251; validation accuracy:0.770425660681864\n",
            "parameters:(70,gini,5); trainining accuracy:0.8186037126715093; validation accuracy:0.7924147669961671\n",
            "parameters:(70,gini,6); trainining accuracy:0.818048829701372; validation accuracy:0.7918095622352229\n",
            "parameters:(70,gini,7); trainining accuracy:0.8385794995964487; validation accuracy:0.8029049828525318\n",
            "parameters:(70,gini,8); trainining accuracy:0.8437752219531881; validation accuracy:0.7992737542868671\n",
            "parameters:(70,gini,9); trainining accuracy:0.8591101694915254; validation accuracy:0.8107726447448054\n",
            "parameters:(70,gini,10); trainining accuracy:0.8706113801452785; validation accuracy:0.8037119225337906\n",
            "parameters:(70,entropy,1); trainining accuracy:0.7182707828894269; validation accuracy:0.7040548718983256\n",
            "parameters:(70,entropy,2); trainining accuracy:0.7483857949959645; validation accuracy:0.7383498083518257\n",
            "parameters:(70,entropy,3); trainining accuracy:0.7870762711864406; validation accuracy:0.7712326003631229\n",
            "parameters:(70,entropy,4); trainining accuracy:0.8037227602905569; validation accuracy:0.7869679241476699\n",
            "parameters:(70,entropy,5); trainining accuracy:0.8161319612590799; validation accuracy:0.7843453701835787\n",
            "parameters:(70,entropy,6); trainining accuracy:0.8365112994350282; validation accuracy:0.8033084526931612\n",
            "parameters:(70,entropy,7); trainining accuracy:0.8395379338175948; validation accuracy:0.8059310066572524\n",
            "parameters:(70,entropy,8); trainining accuracy:0.8334846650524617; validation accuracy:0.7932217066774259\n",
            "parameters:(70,entropy,9); trainining accuracy:0.85204802259887; validation accuracy:0.8065362114181965\n",
            "parameters:(70,entropy,10); trainining accuracy:0.8596146085552865; validation accuracy:0.8018963082509583\n",
            "parameters:(80,gini,1); trainining accuracy:0.7181698950766747; validation accuracy:0.7137381480734315\n",
            "parameters:(80,gini,2); trainining accuracy:0.7670500403551251; validation accuracy:0.7559007464192051\n",
            "parameters:(80,gini,3); trainining accuracy:0.7904055690072639; validation accuracy:0.7744603590881581\n",
            "parameters:(80,gini,4); trainining accuracy:0.799636803874092; validation accuracy:0.7839419003429494\n",
            "parameters:(80,gini,5); trainining accuracy:0.825363196125908; validation accuracy:0.7893887431914465\n",
            "parameters:(80,gini,6); trainining accuracy:0.8243543179983858; validation accuracy:0.7940286463586846\n",
            "parameters:(80,gini,7); trainining accuracy:0.840042372881356; validation accuracy:0.7962477304821465\n",
            "parameters:(80,gini,8); trainining accuracy:0.8481638418079096; validation accuracy:0.80068589872907\n",
            "parameters:(80,gini,9); trainining accuracy:0.8599677158999193; validation accuracy:0.8012911034900141\n",
            "parameters:(80,gini,10); trainining accuracy:0.8663236481033091; validation accuracy:0.8113778495057494\n",
            "parameters:(80,entropy,1); trainining accuracy:0.7517655367231638; validation accuracy:0.7444018559612668\n",
            "parameters:(80,entropy,2); trainining accuracy:0.748133575464084; validation accuracy:0.7377446035908816\n",
            "parameters:(80,entropy,3); trainining accuracy:0.7884887005649718; validation accuracy:0.7686100463990316\n",
            "parameters:(80,entropy,4); trainining accuracy:0.8057405165456013; validation accuracy:0.7847488400242082\n",
            "parameters:(80,entropy,5); trainining accuracy:0.8083131557707829; validation accuracy:0.7744603590881581\n",
            "parameters:(80,entropy,6); trainining accuracy:0.820318805488297; validation accuracy:0.7964494654024612\n",
            "parameters:(80,entropy,7); trainining accuracy:0.8437752219531881; validation accuracy:0.8029049828525318\n",
            "parameters:(80,entropy,8); trainining accuracy:0.8472558514931396; validation accuracy:0.8031067177728465\n",
            "parameters:(80,entropy,9); trainining accuracy:0.8511400322841001; validation accuracy:0.7990720193665524\n",
            "parameters:(80,entropy,10); trainining accuracy:0.8664749798224375; validation accuracy:0.8164212225136172\n",
            "parameters:(90,gini,1); trainining accuracy:0.7262913640032284; validation accuracy:0.7119225337905991\n",
            "parameters:(90,gini,2); trainining accuracy:0.770682001614205; validation accuracy:0.751260843251967\n",
            "parameters:(90,gini,3); trainining accuracy:0.7987792574656981; validation accuracy:0.785555779705467\n",
            "parameters:(90,gini,4); trainining accuracy:0.804227199354318; validation accuracy:0.7827314908210611\n",
            "parameters:(90,gini,5); trainining accuracy:0.8246569814366425; validation accuracy:0.7960459955618318\n",
            "parameters:(90,gini,6); trainining accuracy:0.8344430992736077; validation accuracy:0.8025015130119023\n",
            "parameters:(90,gini,7); trainining accuracy:0.8489205004035513; validation accuracy:0.8123865241073229\n",
            "parameters:(90,gini,8); trainining accuracy:0.8534604519774012; validation accuracy:0.8111761145854347\n",
            "parameters:(90,gini,9); trainining accuracy:0.8636501210653753; validation accuracy:0.8061327415775671\n",
            "parameters:(90,gini,10); trainining accuracy:0.866273204196933; validation accuracy:0.8010893685696994\n",
            "parameters:(90,entropy,1); trainining accuracy:0.7323950766747377; validation accuracy:0.7246318337704256\n",
            "parameters:(90,entropy,2); trainining accuracy:0.7795096852300242; validation accuracy:0.7605406495864434\n",
            "parameters:(90,entropy,3); trainining accuracy:0.7920702179176755; validation accuracy:0.7780915876538229\n",
            "parameters:(90,entropy,4); trainining accuracy:0.7994854721549637; validation accuracy:0.7671979019568287\n",
            "parameters:(90,entropy,5); trainining accuracy:0.8184523809523809; validation accuracy:0.7839419003429494\n",
            "parameters:(90,entropy,6); trainining accuracy:0.8315677966101694; validation accuracy:0.7976598749243494\n",
            "parameters:(90,entropy,7); trainining accuracy:0.8475080710250201; validation accuracy:0.8113778495057494\n",
            "parameters:(90,entropy,8); trainining accuracy:0.8529055690072639; validation accuracy:0.8000806939681259\n",
            "parameters:(90,entropy,9); trainining accuracy:0.8642554479418886; validation accuracy:0.8196489812386524\n",
            "parameters:(90,entropy,10); trainining accuracy:0.8641041162227603; validation accuracy:0.7988702844462376\n",
            "parameters:(100,gini,1); trainining accuracy:0.7266949152542372; validation accuracy:0.7038531369780109\n",
            "parameters:(100,gini,2); trainining accuracy:0.7710855528652139; validation accuracy:0.7583215654629817\n",
            "parameters:(100,gini,3); trainining accuracy:0.7974677158999193; validation accuracy:0.7827314908210611\n",
            "parameters:(100,gini,4); trainining accuracy:0.7984765940274415; validation accuracy:0.777284647972564\n",
            "parameters:(100,gini,5); trainining accuracy:0.8266747376916869; validation accuracy:0.7984668146056082\n",
            "parameters:(100,gini,6); trainining accuracy:0.831315577078289; validation accuracy:0.7966512003227759\n",
            "parameters:(100,gini,7); trainining accuracy:0.8423123486682809; validation accuracy:0.802098043171273\n",
            "parameters:(100,gini,8); trainining accuracy:0.8592110573042776; validation accuracy:0.8107726447448054\n",
            "parameters:(100,gini,9); trainining accuracy:0.8645076674737692; validation accuracy:0.8148073431510995\n",
            "parameters:(100,gini,10); trainining accuracy:0.8750504439063761; validation accuracy:0.8198507161589671\n",
            "parameters:(100,entropy,1); trainining accuracy:0.7337570621468926; validation accuracy:0.7252370385313698\n",
            "parameters:(100,entropy,2); trainining accuracy:0.7774919289749799; validation accuracy:0.7653822876739964\n",
            "parameters:(100,entropy,3); trainining accuracy:0.7992332526230831; validation accuracy:0.7793019971757111\n",
            "parameters:(100,entropy,4); trainining accuracy:0.8247578692493946; validation accuracy:0.7978616098446641\n",
            "parameters:(100,entropy,5); trainining accuracy:0.8208736884584342; validation accuracy:0.7893887431914465\n",
            "parameters:(100,entropy,6); trainining accuracy:0.8258676351896691; validation accuracy:0.8002824288884406\n",
            "parameters:(100,entropy,7); trainining accuracy:0.835905972558515; validation accuracy:0.7994754892071818\n",
            "parameters:(100,entropy,8); trainining accuracy:0.8481133979015335; validation accuracy:0.7944321161993141\n",
            "parameters:(100,entropy,9); trainining accuracy:0.851594027441485; validation accuracy:0.8085535606213435\n",
            "parameters:(100,entropy,10); trainining accuracy:0.8641545601291364; validation accuracy:0.8043171272947347\n"
          ]
        }
      ],
      "source": [
        "for n_estimator in n_estimators:\n",
        "  for crit in criterion:\n",
        "    for m in max_depth:\n",
        "      #instantaite the model with number of estimators as parameter \n",
        "      model =  RandomForestClassifier(n_estimators = n_estimator,criterion=crit,max_depth=m)\n",
        "\n",
        "      # train the model\n",
        "      model.fit(X_train_vec,y_train)\n",
        "\n",
        "      # predict using training and validation \n",
        "      train_pred = model.predict(X_train_vec)\n",
        "      val_pred =  model.predict(X_val_vec)\n",
        "\n",
        "      #accuracy \n",
        "      training_acc = sum(train_pred==y_train)*1.0/len(y_train)\n",
        "      validation_acc =  sum(val_pred==y_val)*1.0/len(y_val)\n",
        "\n",
        "      # results \n",
        "      print(\"parameters:({},{},{}); trainining accuracy:{}; validation accuracy:{}\".format(n_estimator,crit,m,training_acc,validation_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8aQ6bmkH0er"
      },
      "source": [
        "gini performs better than entropy in terms of criterion, the model performs well as we increase n_estimators and max_depth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KU60Mjy4ICQU",
        "outputId": "06c3b1e5-9fe7-4bc9-ffb2-ca842ce16276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_accuracy: 0.9961158192090396  testing_accuracy: 0.8452884964315955\n"
          ]
        }
      ],
      "source": [
        "# let's pass other parameters and test the model on testing dataset \n",
        "model_ = RandomForestClassifier(n_estimators = 500, criterion ='gini',max_depth = 50)\n",
        "\n",
        "model_.fit(X_train_vec,y_train)\n",
        "\n",
        "train_prediction =  model_.predict(X_train_vec)\n",
        "\n",
        "test_prediction =  model_.predict(X_test_vec)\n",
        "\n",
        "training_acc = sum(train_prediction==y_train)*1.0/len(y_train)\n",
        "testing_acc =  sum(test_prediction==testing_dataset['sentiment'])*1.0/len(testing_dataset['sentiment'])\n",
        "\n",
        "print('training_accuracy: {}  testing_accuracy: {}'.format(training_acc,testing_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sFGXNI93gKcO",
        "outputId": "48fc398e-3edc-44da-9185-0c2060dace71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 24781 entries, 25000 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     24781 non-null  object\n",
            " 1   sentiment  24781 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 580.8+ KB\n"
          ]
        }
      ],
      "source": [
        "training_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2tQgNg7VNm91",
        "outputId": "21c16e6e-6cdf-4089-ead8-a7b13cf8c071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/NLP Fellowship /Models/RandomForest.pkl']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model serialization \n",
        "joblib.dump(model_,'/content/drive/MyDrive/NLP Fellowship /Models/RandomForest.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqUsQSXTp-r"
      },
      "source": [
        "## Tuning Linear regression and MLP using pytorch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYmOa4FceYA"
      },
      "source": [
        "### Linear Regression(LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5UcN2hs9TsOk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Dataset, Example\n",
        "from torchtext.legacy.data import BucketIterator\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-imHsbmnfLQx",
        "outputId": "cb4ffd36-4a47-4207-ab05-bb5793b30c36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True # Check this\n",
        "max_document_length = 300 #hyperparameter\n",
        "\n",
        "TEXT = data.Field(lower=True, include_lengths=True,  tokenize='spacy',batch_first=True,  fix_length=max_document_length)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, fields: list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [\n",
        "                Example.fromlist(list(r), fields) \n",
        "                for i, r in df.iterrows()\n",
        "            ], \n",
        "            fields\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cFpjtFdifqql"
      },
      "outputs": [],
      "source": [
        "torch_valid_dataset, torch_test_dataset = DataFrameDataset(\n",
        "    df=testing_dataset, \n",
        "    fields=(\n",
        "        ('review', TEXT),\n",
        "        ('sentiment', LABEL)\n",
        "    )\n",
        ").split() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lFYwOqWyfrpX"
      },
      "outputs": [],
      "source": [
        "torch_train_dataset = DataFrameDataset(\n",
        "    df=training_dataset, \n",
        "    fields=(\n",
        "        ('review', TEXT),\n",
        "        ('sentiment', LABEL)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GhotLBHT4k1c"
      },
      "outputs": [],
      "source": [
        "max_size = 30000 #hyperparameter\n",
        "TEXT.build_vocab(torch_train_dataset, max_size=max_size,vectors='fasttext.simple.300d')\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "BATCH_SIZE = 40 #hyperparameter\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (torch_train_dataset, torch_valid_dataset, torch_test_dataset), \n",
        "    batch_size = BATCH_SIZE ,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "    sort_within_batch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mDuOSj7d4xH-"
      },
      "outputs": [],
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,hidden_size2,hidden_size3, num_classes):\n",
        "        super(LR, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3) \n",
        "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        text = text.float() # dense layer deals just with float type data\n",
        "        x = self.fc1(text) #(m x n) with (n x p)\n",
        "        x = self.relu(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        \n",
        "        preds = self.fc4(x) # crossentropyloss handles the softmax\n",
        "        # preds = F.softmax(preds,1) # nn.softmax\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "84O3_l5y48SO"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size2, hidden_size3, hidden_size4, output_dim, dropout, max_document_length):\n",
        "        super().__init__()\n",
        "        # embedding and convolution layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(embed_size*max_document_length, hidden_size2)  # dense layer\n",
        "        self.fc2 = nn.Linear(hidden_size2, hidden_size3)  # dense layer\n",
        "        self.fc3 = nn.Linear(hidden_size3, hidden_size4)  # dense layer\n",
        "        self.fc4 = nn.Linear(hidden_size4, output_dim)  # dense layer\n",
        "\n",
        "    def forward(self, text):\n",
        "         # text shape = (batch_size, num_sequences)\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "        \n",
        "        x = embedded.view(embedded.shape[0], -1)  # x = Flatten()(x)\n",
        "        #embedded = embedded.unsqueeze(1) # fc gets 4 dimension\n",
        "        \n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        preds = self.fc4(x)\n",
        "        # preds = F.softmax(preds, 1)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dx0mOgTh5cp6"
      },
      "outputs": [],
      "source": [
        "LRlr = 1e-3\n",
        "LRbatch_size = 100\n",
        "LRdropout_keep_prob = 0.3\n",
        "LRembedding_size = 300\n",
        "LRmax_document_length = 300 # each sentence has until 100 words\n",
        "LRvocab_size = len(TEXT.vocab)\n",
        "LRdev_size = 0.8 # split percentage to train\\validation data\n",
        "LRmax_size = 30000 # maximum vocabulary size\n",
        "LRseed = 30\n",
        "LRnum_classes = 2\n",
        "\n",
        "LRnum_epochs = 20\n",
        "LRhidden_size = 300\n",
        "LRhidden_size1 = 400\n",
        "LRhidden_size2 = 200\n",
        "LRhidden_size3 = 80\n",
        "\n",
        "to_train = True\n",
        "LinearRmodel = LR(LRmax_document_length, LRhidden_size,LRhidden_size2,LRhidden_size3, LRnum_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "stn5mSJI5A57"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "batch_size = 100\n",
        "dropout_keep_prob = 0.3\n",
        "embedding_size = 300\n",
        "max_document_length = 300 # each sentence has until 100 words\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "max_size = 30000 # maximum vocabulary size\n",
        "seed = 30\n",
        "num_classes = 2\n",
        "\n",
        "num_epochs = 15\n",
        "hidden_size = 256\n",
        "hidden_size1 = 300\n",
        "hidden_size2 = 128\n",
        "hidden_size3 = 64\n",
        "\n",
        "to_train = True\n",
        "MLP_model = MLP(vocab_size, embedding_size, hidden_size1, hidden_size2, hidden_size3,  num_classes, dropout_keep_prob, max_document_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x0V6quuW5rfb"
      },
      "outputs": [],
      "source": [
        "def accuracy(probs, target):\n",
        "  winners = probs.argmax(dim=1)\n",
        "  corrects = (winners == target)\n",
        "  accuracy = corrects.sum().float() / float(target.size(0))\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8SBm53w6xwh",
        "outputId": "7400d0af-d258-4a5a-bbb4-1ef35b7ed239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.736 | Train Acc: 50.45%\n",
            "\t Val. Loss: 0.692 |  Val. Acc: 50.98%\n",
            "\tTrain Loss: 0.649 | Train Acc: 57.34%\n",
            "\t Val. Loss: 0.689 |  Val. Acc: 53.50%\n",
            "\tTrain Loss: 0.547 | Train Acc: 63.12%\n",
            "\t Val. Loss: 0.801 |  Val. Acc: 52.84%\n",
            "\tTrain Loss: 0.526 | Train Acc: 62.24%\n",
            "\t Val. Loss: 0.952 |  Val. Acc: 54.25%\n",
            "\tTrain Loss: 0.515 | Train Acc: 63.58%\n",
            "\t Val. Loss: 0.855 |  Val. Acc: 52.77%\n",
            "\tTrain Loss: 0.517 | Train Acc: 63.37%\n",
            "\t Val. Loss: 0.941 |  Val. Acc: 53.21%\n",
            "\tTrain Loss: 0.511 | Train Acc: 64.07%\n",
            "\t Val. Loss: 1.019 |  Val. Acc: 53.14%\n",
            "\tTrain Loss: 0.524 | Train Acc: 63.23%\n",
            "\t Val. Loss: 1.028 |  Val. Acc: 52.94%\n",
            "\tTrain Loss: 0.518 | Train Acc: 63.88%\n",
            "\t Val. Loss: 0.790 |  Val. Acc: 53.83%\n",
            "\tTrain Loss: 0.504 | Train Acc: 64.44%\n",
            "\t Val. Loss: 1.023 |  Val. Acc: 53.89%\n",
            "\tTrain Loss: 0.488 | Train Acc: 67.57%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 54.51%\n",
            "\tTrain Loss: 0.452 | Train Acc: 72.59%\n",
            "\t Val. Loss: 0.854 |  Val. Acc: 70.75%\n",
            "\tTrain Loss: 0.284 | Train Acc: 86.98%\n",
            "\t Val. Loss: 0.788 |  Val. Acc: 75.89%\n",
            "\tTrain Loss: 0.136 | Train Acc: 94.63%\n",
            "\t Val. Loss: 1.131 |  Val. Acc: 69.86%\n",
            "\tTrain Loss: 0.057 | Train Acc: 97.82%\n",
            "\t Val. Loss: 1.131 |  Val. Acc: 74.74%\n",
            "\tTrain Loss: 0.038 | Train Acc: 98.66%\n",
            "\t Val. Loss: 1.279 |  Val. Acc: 77.33%\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
            "\t Val. Loss: 1.533 |  Val. Acc: 77.81%\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.74%\n",
            "\t Val. Loss: 1.910 |  Val. Acc: 77.27%\n",
            "\tTrain Loss: 0.011 | Train Acc: 99.67%\n",
            "\t Val. Loss: 1.244 |  Val. Acc: 77.68%\n",
            "\tTrain Loss: 0.017 | Train Acc: 99.48%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 77.04%\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "optimizer = torch.optim.Adam(MLP_model.parameters(), lr=lr)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(LRnum_epochs):\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  for batch in train_iterator:\n",
        "      optimizer.zero_grad()\n",
        "      # retrieve text and no. of words\n",
        "      text, text_lengths = batch.review\n",
        "\n",
        "      #feedforward\n",
        "      # model.to(device)\n",
        "      predictions = MLP_model(text).squeeze(1)\n",
        "      \n",
        "      \n",
        "      loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "      acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "      # perform backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_epoch_loss += loss.item()\n",
        "      train_epoch_acc += acc.item()\n",
        "\n",
        "  \n",
        "\n",
        "  valid_epoch_loss = 0\n",
        "  valid_epoch_acc = 0\n",
        "\n",
        "  MLP_model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in valid_iterator:\n",
        "          text, text_lengths = batch.review\n",
        "\n",
        "          predictions = MLP_model(text).squeeze(1)\n",
        "\n",
        "          loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "          acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "          valid_epoch_loss += loss.item()\n",
        "          valid_epoch_acc += acc.item()\n",
        "\n",
        "   \n",
        "\n",
        "  if valid_epoch_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_epoch_loss\n",
        "            torch.save(MLP_model.state_dict(), 'saved_weights'+'_linear.pt')\n",
        "\n",
        "  print(f'\\tTrain Loss: {train_epoch_loss / len(train_iterator):.3f} | Train Acc: {train_epoch_acc  / len(train_iterator)* 100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_epoch_loss / len(valid_iterator):.3f} |  Val. Acc: {valid_epoch_acc / len(valid_iterator) * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-9jrptFTEWQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa4c8bc-06cb-4c2f-b33c-0cef6c07b4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 0.696 | Train Acc: 50.17%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.10%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.65%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.84%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.67%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.92%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.35%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.12%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.25%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.97%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.82%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.68%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.27%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.85%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.93%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.90%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.03%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.06%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.08%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.49%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.71%\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "optimizer = torch.optim.Adam(LinearRmodel.parameters(), lr=lr)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(LRnum_epochs):\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  for batch in train_iterator:\n",
        "      optimizer.zero_grad()\n",
        "      # retrieve text and no. of words\n",
        "      text, text_lengths = batch.review\n",
        "\n",
        "      #feedforward\n",
        "      # model.to(device)\n",
        "      predictions = LinearRmodel(text).squeeze(1)\n",
        "      \n",
        "      \n",
        "      loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "      acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "      # perform backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_epoch_loss += loss.item()\n",
        "      train_epoch_acc += acc.item()\n",
        "\n",
        "  \n",
        "\n",
        "  valid_epoch_loss = 0\n",
        "  valid_epoch_acc = 0\n",
        "\n",
        "  LinearRmodel.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in valid_iterator:\n",
        "          text, text_lengths = batch.review\n",
        "\n",
        "          predictions = LinearRmodel(text).squeeze(1)\n",
        "\n",
        "          loss = loss_func(predictions, batch.sentiment)\n",
        "\n",
        "          acc = accuracy(predictions, batch.sentiment)\n",
        "\n",
        "          valid_epoch_loss += loss.item()\n",
        "          valid_epoch_acc += acc.item()\n",
        "\n",
        "   \n",
        "\n",
        "  if valid_epoch_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_epoch_loss\n",
        "            torch.save(LinearRmodel.state_dict(), 'saved_weights'+'_linear.pt')\n",
        "\n",
        "  print(f'\\tTrain Loss: {train_epoch_loss / len(train_iterator):.3f} | Train Acc: {train_epoch_acc  / len(train_iterator)* 100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_epoch_loss / len(valid_iterator):.3f} |  Val. Acc: {valid_epoch_acc / len(valid_iterator) * 100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}