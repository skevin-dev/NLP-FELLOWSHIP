{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skevin-dev/NLP-FELLOWSHIP/blob/week1/Word_and_PDF_Day3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Word Documents and PDFs\n",
        "\n",
        "In the African continent, most documents are in word or PDF. For the data in the documents to be processed and used in AI, they need to be extracted. To do this, the data will be extracted to txt format.\n",
        "\n"
      ],
      "metadata": {
        "id": "-1LUjCesZBKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package\n",
        "The package to be used in textract.\n",
        "\n",
        "To install the package, use the commands below\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "pip install textract\n",
        "```\n",
        "\n",
        "The package supports a lot of formats including but not limmited:\n",
        "\n",
        "csv, doc, docx, epub,json, html,pdf, jpg, pptx, xls, xlsx, ogg \n",
        "\n"
      ],
      "metadata": {
        "id": "YhWTdFi4ebY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "42mhUB5sSWCU",
        "outputId": "802a17a7-9e2c-4a4e-c3ce-3b4152efb0d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 64.7 MB/s \n",
            "\u001b[?25hCollecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 45.9 MB/s \n",
            "\u001b[?25hCollecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.9.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tzdata\n",
            "  Downloading tzdata-2022.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 66.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=773c11f5aa4ab4bd70f07859b1b74875d74b8708fa4515e00b1a0d3eeed20458\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=8beea60319011f6c95ec0599a8a0a2777bb0aef9dbe34c0daea145844bcb567f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=38d1b11e51679545fa3e03ace912f0ddf2940f1b6cfc519032e015f365c367c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=d1f97f7db0de90fc076116a7eb0bb7dfefa99623864bc073be75e44147eaaf9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, soupsieve, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 soupsieve-2.3.2.post1 textract-1.6.5 tzdata-2022.5 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rx5KhiWzQVRQ"
      },
      "outputs": [],
      "source": [
        "import textract\n",
        "import os\n",
        "import re "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jLkA4whSJzO",
        "outputId": "934d9402-14ad-462a-d9e4-7fd4531f7ade"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wil first mount the google drive. This will give us acces to my drive folder where we can find files"
      ],
      "metadata": {
        "id": "4n65qBvfljQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting text with python package\n",
        "Docx files are writen in xml under the hood. If you unzip a docx file, you will find the components. The text is found in an xml file named document.xml. From the xml, you can extract text using beautifulsoup."
      ],
      "metadata": {
        "id": "xKU2FIS4vK4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from bs4 import BeautifulSoup as bs\n",
        "# content = []\n",
        "\n",
        "# xml = 'document.xml'\n",
        "\n",
        "# with open(xml, \"r\") as file:\n",
        "#     # Read each line in the file, readlines() returns a list of lines\n",
        "#     content = file.readlines()\n",
        "#     # Combine the lines in the list into a string\n",
        "#     content = \"\".join(content)\n",
        "#     bs_content = bs(content, \"lxml\")\n",
        "\n",
        "# print(bs_content.prettify())\n"
      ],
      "metadata": {
        "id": "TZ1MqQnWcLXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In class practicals\n",
        "From the xml, extract the text. The text should be put in a text file and have each sentence should be in its own line (No spaces between each line). e.g\n",
        "\n",
        "this is a book.\n",
        "\n",
        "the book looks nice.\n"
      ],
      "metadata": {
        "id": "KDcIOuhav13T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from bs4 import BeautifulSoup as bs\n",
        "# content = []\n",
        "# xml = 'document.xml'\n",
        "\n",
        "# with open(xml, \"r\") as file:\n",
        "#     # Read each line in the file, readlines() returns a list of lines\n",
        "#     content = file.readlines()\n",
        "#     # Combine the lines in the list into a string\n",
        "#     content = \"\".join(content)\n",
        "#     bs_content = bs(content, \"lxml\")\n",
        "\n",
        "# print(bs_content.prettify())"
      ],
      "metadata": {
        "id": "2YjOQckOakey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "You will be given 5 parliamentary proceedings, extract the text and put them in one txt file. Ski[p the last and first page. Steps:\n",
        "\n",
        "\n",
        "1.   Get a list of all the filepaths (through code)\n",
        "2.   open the file you want to write to(Note, remember to append the text)\n",
        "3.   Loop through every file\n",
        "4.   extract the text\n",
        "5.   write to the file (each sentence will be on its own line). There should be no spaces between lines (Note: remove empty lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "ycw-85mfyM_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Change the ditection to where the files are located\n",
        "path = '/content/drive/MyDrive/NLP Fellowship /week1 /data/'\n",
        "os.chdir(path)\n",
        "!ls"
      ],
      "metadata": {
        "id": "KMAEBzOkqSEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e401bc-079b-493e-a70a-d260cfb79586"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hansard Report - Thursday, 13th October 2022 (P).pdf'\n",
            "'Hansard Report - Thursday 6th October 2022 (P).pdf'\n",
            "'Hansard Report - Tuesday, 11th October 2022 (P).pdf'\n",
            "'Hansard Report - Wednesday, 12th October 2022 (A).pdf'\n",
            "'Hansard Report - Wednesday, 12th October 2022 (P)_0.pdf'\n",
            " text_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_path = []\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        f_path=os.path.join(path + file)\n",
        "        file_path.append(f_path)\n",
        "file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyXh_FB0aK-4",
        "outputId": "d4364403-c94a-4098-a4a8-6fa0405c08d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/NLP Fellowship /week1 /data/Hansard Report - Thursday, 13th October 2022 (P).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/Hansard Report - Wednesday, 12th October 2022 (P)_0.pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/Hansard Report - Wednesday, 12th October 2022 (A).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/Hansard Report - Tuesday, 11th October 2022 (P).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/Hansard Report - Thursday 6th October 2022 (P).pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # code for extracting text pdf\n",
        "for fpth in file_path:\n",
        "  text = textract.process(fpth)\n",
        "  text = text.decode(\"utf-8\")\n",
        "\n",
        "  y = open(path+'/text_files/final_file1.txt', 'w+',  encoding='utf-8')\n",
        "  for x in text.split('\\n'): #get every single line\n",
        "      if x != '':\n",
        "          y.write(x+' \\n')"
      ],
      "metadata": {
        "id": "B5reRJOraVXY"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}