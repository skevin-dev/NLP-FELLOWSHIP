{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skevin-dev/NLP-FELLOWSHIP/blob/week1/Word_and_PDF_Day3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Word Documents and PDFs\n",
        "\n",
        "In the African continent, most documents are in word or PDF. For the data in the documents to be processed and used in AI, they need to be extracted. To do this, the data will be extracted to txt format.\n",
        "\n"
      ],
      "metadata": {
        "id": "-1LUjCesZBKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package\n",
        "The package to be used in textract.\n",
        "\n",
        "To install the package, use the commands below\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "pip install textract\n",
        "```\n",
        "\n",
        "The package supports a lot of formats including but not limmited:\n",
        "\n",
        "csv, doc, docx, epub,json, html,pdf, jpg, pptx, xls, xlsx, ogg \n",
        "\n"
      ],
      "metadata": {
        "id": "YhWTdFi4ebY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "42mhUB5sSWCU",
        "outputId": "25f72cc3-b2e9-4e39-87ed-dd4c06de03e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 91.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 69.2 MB/s \n",
            "\u001b[?25hCollecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 42.5 MB/s \n",
            "\u001b[?25hCollecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 46.0 MB/s \n",
            "\u001b[?25hCollecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 44.0 MB/s \n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.9.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting tzdata\n",
            "  Downloading tzdata-2022.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 73.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=050f1bdbb60e9ad5418df1a9534b18a911ea23595a10b755a375ccfa0bbc5c22\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=32447c5b079b17a6971df0ac3a30c547cb1e3d666cdf0e39e3756ea21a6daba3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=ecb334a5244805b7719b3c0015b88ef197344ac46445cfc230bc855f12a8dbb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=05a174789be1196daba350f7c7a6dbd68b9c5a93c8c3a6049290d81d771677e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, soupsieve, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 soupsieve-2.3.2.post1 textract-1.6.5 tzdata-2022.5 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XORBMHJcSW5z",
        "outputId": "82d4921d-c009-448a-b105-72cda49ad903"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf2\n",
            "  Downloading PyPDF2-2.11.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from pypdf2) (4.1.1)\n",
            "Installing collected packages: pypdf2\n",
            "Successfully installed pypdf2-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rx5KhiWzQVRQ"
      },
      "outputs": [],
      "source": [
        "import textract\n",
        "import os\n",
        "import re \n",
        "import PyPDF2 as pd2\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jLkA4whSJzO",
        "outputId": "f00845d0-7b92-49a9-90dd-5c3c5305c580"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wil first mount the google drive. This will give us acces to my drive folder where we can find files"
      ],
      "metadata": {
        "id": "4n65qBvfljQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting text with python package\n",
        "Docx files are writen in xml under the hood. If you unzip a docx file, you will find the components. The text is found in an xml file named document.xml. From the xml, you can extract text using beautifulsoup."
      ],
      "metadata": {
        "id": "xKU2FIS4vK4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In class practicals\n",
        "From the xml, extract the text. The text should be put in a text file and have each sentence should be in its own line (No spaces between each line). e.g\n",
        "\n",
        "this is a book.\n",
        "\n",
        "the book looks nice.\n"
      ],
      "metadata": {
        "id": "KDcIOuhav13T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "You will be given 5 parliamentary proceedings, extract the text and put them in one txt file. Ski[p the last and first page. Steps:\n",
        "\n",
        "\n",
        "1.   Get a list of all the filepaths (through code)\n",
        "2.   open the file you want to write to(Note, remember to append the text)\n",
        "3.   Loop through every file\n",
        "4.   extract the text\n",
        "5.   write to the file (each sentence will be on its own line). There should be no spaces between lines (Note: remove empty lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "ycw-85mfyM_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access the directory of the data and extract files "
      ],
      "metadata": {
        "id": "ygFJcvFHWJ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Change the ditection to where the files are located\n",
        "path = '/content/drive/MyDrive/NLP Fellowship /week1 /data/'\n",
        "os.chdir(path)\n",
        "!ls"
      ],
      "metadata": {
        "id": "KMAEBzOkqSEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb72f943-535d-4e7a-f5f9-3ae0d87faa9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hansard Report - Thursday, 13th October 2022 (P).pdf'\n",
            "'Hansard Report - Thursday 6th October 2022 (P).pdf'\n",
            "'Hansard Report - Tuesday, 11th October 2022 (P).pdf'\n",
            "'Hansard Report - Wednesday, 12th October 2022 (A).pdf'\n",
            "'Hansard Report - Wednesday, 12th October 2022 (P)_0.pdf'\n",
            " text_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting pdf file only \n",
        "doc_dir = glob.glob('*.pdf')\n",
        "doc_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT7PzfQNS7Po",
        "outputId": "b8c2f3b1-e72b-4ffc-b336-66b9476fb102"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hansard Report - Thursday, 13th October 2022 (P).pdf',\n",
              " 'Hansard Report - Wednesday, 12th October 2022 (P)_0.pdf',\n",
              " 'Hansard Report - Wednesday, 12th October 2022 (A).pdf',\n",
              " 'Hansard Report - Tuesday, 11th October 2022 (P).pdf',\n",
              " 'Hansard Report - Thursday 6th October 2022 (P).pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove the first and the last pages "
      ],
      "metadata": {
        "id": "7ShnbJ7aWClS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to drop pages \n",
        "def drop_pages(path):\n",
        "  file_original = pd2.PdfFileReader(path)    #Open the Original file containing all the pages\n",
        "  last_page = file_original.getNumPages()-1\n",
        "  file_new = pd2.PdfFileWriter()\n",
        "  for page_number in range(1, last_page):\n",
        "    page_content = file_original.getPage(page_number)\n",
        "    file_new.add_page(page_content)\n",
        "  return file_new"
      ],
      "metadata": {
        "id": "eyxibOQZT_1I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = drop_pages(path+doc_dir[2])\n",
        "test.getNumPages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1JXU8gTUlQr",
        "outputId": "09ac751f-bb8e-49ec-b5d2-d0b2a519686c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to save cleaned file \n",
        "def save(name, file):\n",
        "  outputfilename = \"/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/\" + name  #construct the path and filename\n",
        "  with open(outputfilename, 'wb') as output:\n",
        "    file.write(output)\n"
      ],
      "metadata": {
        "id": "m8GqHcgKU4kc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in doc_dir:\n",
        "  file_new = drop_pages(path +file)\n",
        "  save(file,file_new)"
      ],
      "metadata": {
        "id": "pyyJPxoVUJwQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a list of all the filepaths"
      ],
      "metadata": {
        "id": "dr0j3dHqW-m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_path = '/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/'\n",
        "file_path = []\n",
        "for file in os.listdir(new_path):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        f_path=os.path.join(new_path + file)\n",
        "        file_path.append(f_path)\n",
        "file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyXh_FB0aK-4",
        "outputId": "8ed96a2c-1d96-4650-d0b1-b42ca39230f4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/Hansard Report - Thursday, 13th October 2022 (P).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/Hansard Report - Wednesday, 12th October 2022 (P)_0.pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/Hansard Report - Wednesday, 12th October 2022 (A).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/Hansard Report - Tuesday, 11th October 2022 (P).pdf',\n",
              " '/content/drive/MyDrive/NLP Fellowship /week1 /data/cleaned_pdfs/Hansard Report - Thursday 6th October 2022 (P).pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop through files, extract text and write to text file"
      ],
      "metadata": {
        "id": "zEy08DWabKEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # code for extracting text pdf\n",
        "for f in file_path:\n",
        "  text_ = textract.process(f)\n",
        "  text_ = text_.decode(\"utf-8\")\n",
        "\n",
        "  y = open(new_path+'/final_file2.txt', 'w+',  encoding='utf-8')\n",
        "  for x in text_.split('\\n'): #get every single line\n",
        "      if x != '':\n",
        "          y.write(x+' \\n')"
      ],
      "metadata": {
        "id": "B5reRJOraVXY"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}